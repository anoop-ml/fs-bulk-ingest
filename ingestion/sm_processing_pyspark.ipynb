{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Ingest Orchestration via SageMaker Processing using PySpark"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.spark.processing import PySparkProcessor"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Application Code using Pyspark to ingest into Feature Store. This is run in the SageMaker Processing Container"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%writefile fs_pyspark_ingest.py\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import boto3\n",
    "import numpy as np\n",
    "from botocore.config import Config\n",
    "\n",
    "\n",
    "def ingest_df_into_fg(feature_group_name, rows):\n",
    "    \n",
    "    boto_session = boto3.Session()\n",
    "    sagemaker_client = boto_session.client(service_name='sagemaker')\n",
    "    featurestore_runtime = boto_session.client(service_name='sagemaker-featurestore-runtime', config=Config(\n",
    "                                         retries = {\n",
    "                                             'max_attempts': 10,\n",
    "                                             'mode': 'standard'\n",
    "                                         }))\n",
    "    rows = list(rows)\n",
    "    total_rows = len(rows)    \n",
    "    if total_rows == 0:\n",
    "        raise (f'Empty data frame')    \n",
    "\n",
    "    for index, row in enumerate(rows):\n",
    "        record = [{\"FeatureName\": column, \"ValueAsString\": str(row[column])} \\\n",
    "                   for column in row.__fields__ if row[column] != None]\n",
    "\n",
    "        resp = featurestore_runtime.put_record(FeatureGroupName=feature_group_name, Record=record)\n",
    "\n",
    "        if not resp['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "            raise (f'PutRecord failed: {resp}')\n",
    "\n",
    "def run_pyspark_job(args):\n",
    "    spark = SparkSession.builder.appName('PySparkJob').getOrCreate()\n",
    "    df = spark.read.options(Header=True).csv(args.s3_uri_prefix)\n",
    "    \n",
    "    if args.custom_partitions:\n",
    "        df = df.repartition(args.custom_partitions)\n",
    "    \n",
    "    df.foreachPartition(lambda rows: ingest_df_into_fg(args.feature_group_name, rows))\n",
    "    \n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--custom_partitions\", type=int)\n",
    "    parser.add_argument(\"--s3_uri_prefix\", type=str, required=True)\n",
    "    parser.add_argument(\"--feature_group_name\", type=str, required=True)\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parse_args()\n",
    "    run_pyspark_job(args)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Please gather the s3 location for chunked files and the Feature Group Name"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "s3_uri_prefix = 's3://fs-pyspark-dbg/data/10M/'\n",
    "feature_group_name = 'ingest-fg-06-17-2021-14-46-44'\n",
    "run_config = '10M'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sample Configs for SageMaker Processing\n",
    "Naming Convention = #RecordsInDataSet"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "instance_configs = {\n",
    "    '10M':{\n",
    "        'instance_type': 'ml.m5.4xlarge',\n",
    "        'instance_count': 8,\n",
    "        'custom_partitions': '512'\n",
    "    },\n",
    "    '1M':{\n",
    "        'instance_type': 'ml.m5.4xlarge',\n",
    "        'instance_count': 8,\n",
    "        'custom_partitions': '128',\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Orchestrate ingestion using SageMaker Processing Job\n",
    "\n",
    "Logs for sagemaker processing jobs (please update your region in the url) - https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/processing-jobs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "pyspark_processor = PySparkProcessor(framework_version='2.4',\n",
    "                                     role=get_execution_role(),\n",
    "                                     instance_type=instance_configs[run_config]['instance_type'],\n",
    "                                     instance_count=instance_configs[run_config]['instance_count'],\n",
    "                                     env={'AWS_DEFAULT_REGION': boto3.Session().region_name})\n",
    "\n",
    "pyspark_processor.run(\n",
    "    submit_app='fs_pyspark_ingest.py',\n",
    "    arguments = ['--s3_uri_prefix', s3_uri_prefix,\n",
    "                 '--feature_group_name', feature_group_name,\n",
    "                 '--custom_partitions', instance_configs[run_config]['custom_partitions']],\n",
    "    spark_event_logs_s3_uri='s3://fs-ingest-2/spark-logs',\n",
    "    logs=False\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Launch PySpark History Server - https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html#spark-history-server"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pyspark_processor.start_history_server()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}