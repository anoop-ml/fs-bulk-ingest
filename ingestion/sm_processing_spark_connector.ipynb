{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "532cd085",
   "metadata": {},
   "source": [
    "## Ingest Orchestration via SageMaker Processing using Spark Connector\n",
    "\n",
    "[spark connector](https://aws.amazon.com/about-aws/whats-new/2022/01/amazon-sagemaker-feature-store-connector-apache-spark-batch-data-ingestion/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9039f7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.spark.processing import PySparkProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d52db669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your acct id\n",
    "tmp = !(aws sts get-caller-identity --query Account --output text)\n",
    "account_id = str(tmp.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7f29bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "instance_configs = {\n",
    "    '1M':{\n",
    "        'instance_type': 'ml.m5.4xlarge',\n",
    "        'instance_count': 8,\n",
    "    }\n",
    "}\n",
    "s3_uri_prefix = 's3://fs-ingest/data/1M/'\n",
    "feature_group_name = 'ingest-fg-02-08-2022-19-17-20'\n",
    "run_config = '1M'\n",
    "region = boto3.Session().region_name\n",
    "image_uri = f'{account_id}.dkr.ecr.{region}.amazonaws.com/sagemaker-spark-main-fs-dbg:3.0-cpu-py37-v1.0' # your ecr image\n",
    "feature_group_arn = f\"arn:aws:sagemaker:{region}:{account_id}:feature-group/{feature_group_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ce726c",
   "metadata": {},
   "source": [
    "#### Application Code using Pyspark to ingest into Feature Store. This is run in the SageMaker Processing Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c2d0123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fs_spark_connector_ingest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fs_spark_connector_ingest.py\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from feature_store_pyspark.FeatureStoreManager import FeatureStoreManager\n",
    "import feature_store_pyspark\n",
    "\n",
    "def run_pyspark_job(args):\n",
    "    \n",
    "    extra_jars = \",\".join(feature_store_pyspark.classpath_jars())\n",
    "    spark = SparkSession.builder \\\n",
    "                        .config(\"spark.jars\", extra_jars) \\\n",
    "                        .getOrCreate()\n",
    "    \n",
    "    df = spark.read.options(Header=True).csv(args.s3_uri_prefix)\n",
    "    \n",
    "    feature_store_manager= FeatureStoreManager()\n",
    "    \n",
    "    feature_store_manager.ingest_data(input_data_frame=df, feature_group_arn=args.feature_group_arn)\n",
    "\n",
    "    \n",
    "    print ('done ingesting')\n",
    "    \n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--s3_uri_prefix\", type=str, required=True)\n",
    "    parser.add_argument(\"--feature_group_arn\", type=str, required=True)\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parse_args()\n",
    "    run_pyspark_job(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de21ced",
   "metadata": {},
   "source": [
    "### Orchestrate ingestion using SageMaker Processing Job\n",
    "\n",
    "Logs for sagemaker processing jobs (please update your region in the url) - https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/processing-jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc473fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "pyspark_processor = PySparkProcessor(image_uri=image_uri,\n",
    "                                     role=get_execution_role(),\n",
    "                                     instance_type=instance_configs[run_config]['instance_type'],\n",
    "                                     instance_count=instance_configs[run_config]['instance_count'],\n",
    "                                     env={'AWS_DEFAULT_REGION': boto3.Session().region_name})\n",
    "\n",
    "pyspark_processor.run(\n",
    "    submit_app='fs_spark_connector_ingest.py',\n",
    "    arguments = ['--s3_uri_prefix', s3_uri_prefix,\n",
    "                 '--feature_group_arn', feature_group_arn],\n",
    "    logs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12a9403",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
